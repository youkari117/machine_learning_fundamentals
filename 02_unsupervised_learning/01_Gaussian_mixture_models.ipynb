{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d361930b-24b7-4cd5-b27e-76321b2f549b",
   "metadata": {},
   "source": [
    "<h1>GaussianMixture</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f87fe9-7440-43eb-a4d7-ec01255ab87e",
   "metadata": {},
   "source": [
    "<h4>概要</h4>\n",
    "混合ガウスモデル（GMM:gaussin mixture model）は、パラメータがわからない複数のガウス分布を混ぜた（足した）ものからインスタンスが生成されていることを前提とする確率的なモデルである。一つのガウス分布から生成された全てのインスタンスは、一般に楕円体のような形になるクラスタを形成する。個々のクラスタは、異なる形、サイズ、密度、向きを持つことができる\n",
    "\n",
    "<h4>GMMの構成</h4>\n",
    "GMMのクラスは複数用意されている。最も単純なGaussianMixtureクラスの場合、ガウス分布の数kをあらかじめ指定しなければならない。データセットXは、次のような確率的プロセスによって生成されたものと仮定される<br>\n",
    "・個々のインスタンスに対し、k個のクラスタから無作為に一つが選択される。j番目のクラスタが選ばれる確率は、クラスタの重み$\\phi^{(j)}$によって定義される。i番目のインスタンスのために選ばれたクラスタのインデックスは、$z^{(i)}$と表記する<br>\n",
    "・$z^{(i)}=j$なら、つまりi番目のインスタンスがj番目のクラスタに振り分けれているなら、このインスタンスの位置${x}^{(i)}$は、平均が$\\mu^{(j)}$、共分散行列が$\\Sigma^{(j)}$のガウス分布から無作為にサンプリングされる。これを$x^{(j)}\\sim N(\\mu^{(j)}, \\Sigma^{(j)})$と記述する<br>\n",
    "要するに、個々の変数$z^{(i)}$は、重みが$\\phi^{(j)}$のカテゴリカル分布から抽出され、個々の変数${x}^{(i)}$は、平均、共分散行列がクラスタ$z^{(i)}$によって定義される正規分布から抽出される<br>\n",
    "ここで${x}^{(i)}$は、その値がわかっているため、観測変数である。対して$z^{(i)}$は、その値がわからないため、潜在変数である\n",
    "\n",
    "<h4>推定しなければならないパラメータ</h4>\n",
    "アルゴリズムが推定しなければならないパラメータは以下の通りである<br>\n",
    "重み$\\phi^{(j)}$、平均$\\mu^{(j)}$、共分散行列$\\Sigma^{(j)}$ $(j=1,\\dots,k)$<br>\n",
    "ここで、$\\mu$と$\\Sigma$はベクトルで、その内包するパラメータ数はガウス分布の次元数による\n",
    "\n",
    "<h4>EMアルゴリズム</h4>\n",
    "sklearnのGaussianMixtureモデルは、EM（期待値最大化）アルゴリズムで、パラメータを推定する。EM法は、最初にクラスタのパラメータを無作為に選び、期待値ステップ（expectation step）と最大化ステップ（maximaization step）の2ステップの処理をパラメータが収束するまで繰り返す。期待値ステップは、各インスタンスが各クラスタに属する確率を現在のクラスタパラメータに基づき推計し、最大化ステップはデータセットに含まれる全てのインスタンスを使って、当該クラスタに属する確率によりインスタンスに重みをつけて、クラスタを更新する<br>\n",
    "EM法は、kmeansと同様に、非最適解に収束することがあるため、何度か実行して最良解を残す必要がある。そのためには、<b>n_init</b>に実行回数を渡せば良い。デフォルトは1のため注意\n",
    "\n",
    "<h4>GMMのメソッド</h4>\n",
    "ハードクラスタリングでは<b>predict()</b>メソッド、ソフトクラスタリングでは<b>predict_proba()</b>メソッドを使う<br>\n",
    "混合ガウスモデルは生成的なモデルであるため、<b>sample()</b>メソッドを使って、モデルから新しいインスタンスをサンプリングすることができる（インスタンスはクラスタインデックス順に並べられる）<br>\n",
    "<b>score_sample()</b>メソッドで任意の位置におけるモデルの密度を推計することもできる。このメソッドは、与えられた各インスタンスについて、その位置の確率密度関数の対数を推計する。そのため、スコアをexponentialの指数として使えば、そのインスタンスの位置の確率密度の値が得られる\n",
    "\n",
    "<h4>モデルに制約を加える</h4>\n",
    "次元が多く、クラスタ数が多く、インスタンスが少ない、といった場合には、EMアルゴリズムは最適解に収束するのに苦労する。そこで、アルゴリズムが学習しなければならないパラメータの数を減らす、つまりモデルに制約をかけることを考える。そのためには、<b>covariance_type</b>パラメータを次の中のどれかにする<br>\n",
    "・\"spherical\"→全てのクラスタが円形でなければならない。ただし、直径はまちまちで良い（つまり、分散は異なっていて良い）<br>\n",
    "・\"diag\"→クラスタは楕円形であればどのような形でもいいし、サイズもまちまちで良い。しかし、楕円体の軸は座標軸と平行でなければならない。つまり、共分散行列は対角行列でなければならない<br>\n",
    "・\"tied\"→全てのクラスタの楕円体の形、サイズ、向きが同じでなければならない。（つまり、全てのクラスタが同じ共分散行列を共有していなければならない）<br>\n",
    "<b>covariance_type</b>のデフォルトは、クラスタごとに形、サイズ、向きがばらばらで良いという意味の\"full\"である（クラスタごとに独自の無制約な共分散行列がある）\n",
    "\n",
    "<h4>GMMによる異常検知</h4>\n",
    "混合ガウスモデルは異常検知にも使用できる。やり方はごく簡単で、密度の低い領域にあるインスタンスを異常値と見做せば良い。ただし、混合ガウスモデルは外れ値を含む全てのデータに適合しようとするので、異常値が多すぎると、モデルの「正常性」の解釈にバイアスがかかり、外れ値の一部が正常値と見なされてしまう。そのような場合は、とりあえずモデルを訓練してみた上で、もっとも極端な外れ値を検知、除去し、少しクリーンアップされたデータセットを改めて訓練すると良い<br>\n",
    "※余談だが、異常検知と新規検知は異なる。新規検知はアルゴリズムが「クリーン」なデータセットで訓練されることが前提となっている点で異常検知と異なる。異常検知にはこのような前提条件はない\n",
    "\n",
    "<h4>クラスタ数の決め方</h4>\n",
    "K平均法と同様に、慣性やシルエットスコアを使うこともできるが、クラスタが円形でなかったり、サイズがまちまちであったりすると、これらの方法は信頼性がないため、混合ガウスモデルではベイズ情報量規準（BIC）や赤池情報量規準（AIC）といった理論的な情報量規準を最小化するモデルを見つける、という方針をとる<br>\n",
    "$BIC=log(m)p-2log(\\hat{L})$<br>\n",
    "$AIC=2p-2log(\\hat{L})$<br>\n",
    "mはインスタンス数、pはモデルが学習するパラメータ数、$\\hat{L}$はモデルの尤度関数<br>\n",
    "BICとAICは、共に学習するパラメータが多い（例えば、クラスタ数が多い）モデルにペナルティを与え、データによく適合するモデルに報酬を与える。両者に差が出る場合、BICが選ぶモデルはAICが選ぶモデルよりも単純（パラメータが少ない）なものの、データへの適合度が低いものになる傾向がある（データセットが大きいときには特にこの特徴が顕著になる）<br>\n",
    "BICとAICの計算には、<b>bic()</b>、<b>aic()</b>メソッドを呼び出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda6c16-5a3e-49f5-8786-5b82f548280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c511cd0-9a71-4644-996b-989780a34761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7148f-76af-485f-9461-c5ac3e76b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X):\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=35, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=2, linewidths=12,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                linewidths=1, colors='k')\n",
    "    plot_data(X)\n",
    "    if show_centroids:\n",
    "        plot_centroids(clusterer.cluster_centers_)\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27bd069-c777-4844-b2f1-255232c8befd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a57c7d89-511d-4771-b79d-a3a543cabf4c",
   "metadata": {},
   "source": [
    "<h1>BayesianGaussianMixture</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07fe0a-05e4-45d4-9533-9d48d01c4be9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75db5db-5b5b-46d6-a04d-67270549a6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3773672-28b9-4556-872f-4f37b03a476e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
